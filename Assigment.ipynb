{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Assignment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = sc.textFile('hdfs://master:9000/yellow_tripdata_1m.csv')\n",
    "vendors = sc.textFile('hdfs://master:9000/yellow_tripvendors_1m.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Average trip duration per hour it started ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def strip_time(time):\n",
    "    return datetime.strptime(time, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trp_dur = trips.map(lambda line: line.split(\",\")[1:3])\\\n",
    "        .map(lambda lst: (strip_time(lst[0]), strip_time(lst[1])))\\\n",
    "        .map(lambda lst: (lst[0].hour, lst[1]-lst[0]))\\\n",
    "        .mapValues(lambda x: (x, 1))\\\n",
    "        .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n",
    "        .map(lambda lst: (lst[0], lst[1][0].total_seconds()/(60*lst[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(23, 13.95847112523272), (22, 14.231797625637356), (21, 13.510855327392882), (20, 13.57589963636424), (19, 14.221208805985727), (18, 15.29045374860119), (17, 16.510825654408613), (16, 17.213072069374675), (15, 30.223498632126276), (14, 16.523138380789476), (13, 15.553918733195129), (12, 15.130881322885683), (11, 14.935821221905567), (10, 14.657939169698276), (9, 14.67010641976562), (8, 14.627504543367822), (7, 13.395006418527384), (6, 12.487420237563239), (5, 13.275583221175415), (4, 13.799857931121963), (3, 13.322282520526887), (2, 13.0356355926767), (1, 13.975069898907133), (0, 14.01779373736224)]\n"
     ]
    }
   ],
   "source": [
    "print(avg_trp_dur.top(24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trp_dur = avg_trp_dur.sortByKey(ascending=True)\\\n",
    "            .map(lambda tpl: {\"HourOfDay\":tpl[0], \"AverageTripTime\":tpl[1]})\n",
    "fieldnames=[\"HourOfDay\", \"AverageTripTime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "def writeRecords(records):\n",
    "    \"\"\"Write out CSV lines\"\"\"\n",
    "    output = StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=fieldnames, delimiter='\\t')\n",
    "    for record in records:\n",
    "        writer.writerow(record)\n",
    "    return [output.getvalue()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/average_trip.csv': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /average_trip.csv \n",
    "avg_trp_dur.mapPartitions(writeRecords).saveAsTextFile(\"hdfs://master:9000/average_trip.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Max amount paid per vendor ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendors_key_val = vendors.map(lambda line: line.split(\",\")[0:2])\\\n",
    "                        .map(lambda lst: (lst[0],lst[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_key_val = trips.map(lambda line: line.split(\",\")[:])\\\n",
    "                    .map(lambda lst: (lst[0],lst[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinRDD = vendors_key_val.join(trips_key_val)\\\n",
    "                    .map(lambda tpl: tpl[1])\\\n",
    "                    .reduceByKey(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2', '99.99'), ('1', '995.3')]\n"
     ]
    }
   ],
   "source": [
    "print(joinRDD.top(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_by_vndr = joinRDD.sortByKey(ascending=True)\\\n",
    "            .map(lambda tpl: {\"VendorID\":tpl[0], \"MaxAmountPaid\":tpl[1]})\n",
    "fieldnames=[\"VendorID\", \"MaxAmountPaid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/max_amount.csv': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /max_amount.csv \n",
    "max_by_vndr.mapPartitions(writeRecords).saveAsTextFile(\"hdfs://master:9000/max_amount.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Machine Learning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = trips.map(lambda line: line.split(\",\")[3:5])\\\n",
    "                .map(lambda lst: array([float(lst[0]), float(lst[1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model = KMeansModel(ml_data.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = KMeans.train(ml_data, 5, maxIterations=3, initializationMode=\"kmeans||\", initialModel=init_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-78.50386634,  40.61111272]),\n",
       " array([-0.0001304 ,  0.00050874]),\n",
       " array([-73.95393671,  40.69903131]),\n",
       " array([-73.99216792,  40.74268109]),\n",
       " array([-73.96113226,  40.77182523])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "centroids = []\n",
    "for c in clusters.clusterCenters:\n",
    "    centroids.append((i, c))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_centroids = sc.parallelize(centroids)\n",
    "p_centroids = p_centroids.map(lambda tpl: {\"ID\":tpl[0], \"Centroid\":tpl[1]})\n",
    "fieldnames=[\"ID\", \"Centroid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/centroids.csv': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /centroids.csv \n",
    "p_centroids.mapPartitions(writeRecords).saveAsTextFile(\"hdfs://master:9000/centroids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Page Rank ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = sc.textFile('hdfs://master:9000/web-Google.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pages.filter(lambda line: line[0]!='#')\\\n",
    "            .map(lambda line: line.split('\\t'))\\\n",
    "            .map(lambda lst: (int(lst[0]), int(lst[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 875713\n",
    "p0 = 0.5\n",
    "d = 0.85\n",
    "\n",
    "constant = 1-d/N\n",
    "\n",
    "iterations = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = edges.distinct().groupByKey().mapValues(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = links.mapValues(lambda v: 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(iterations):    \n",
    "    contribs = links.join(ranks)\\\n",
    "                   .map(lambda tpl: tpl[1])\\\n",
    "                   .flatMap(lambda tpl: map(lambda url: (url, tpl[1]/len(tpl[0])), tpl[0]))\n",
    "    \n",
    "    ranks = contribs.reduceByKey(lambda x,y: x + y).mapValues(lambda v: constant + d * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ranks.sortByKey(ascending=True)\\\n",
    "            .map(lambda tpl: {\"Page ID\":tpl[0], \"PageRank\":tpl[1]})\n",
    "fieldnames=[\"Page ID\", \"PageRank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/pagerank.csv': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /pagerank.csv \n",
    "ranks.mapPartitions(writeRecords).saveAsTextFile(\"hdfs://master:9000/pagerank.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good link for PageRank: https://github.com/abbas-taher/pagerank-example-spark2.0-deep-dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Linear Algebra - Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixA_txt = sc.textFile('hdfs://master:9000/A.csv')\n",
    "matrixB_txt = sc.textFile('hdfs://master:9000/B.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixA = matrixA_txt.map(lambda line: line.split(\",\"))\\\n",
    "                       .map(lambda lst: (int(lst[0]),int(lst[1]),int(lst[2]), 'A'))\n",
    "\n",
    "matrixB = matrixB_txt.map(lambda line: line.split(\",\"))\\\n",
    "                       .map(lambda lst: (int(lst[0]),int(lst[1]),int(lst[2]), 'B'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 3, 'A')\n"
     ]
    }
   ],
   "source": [
    "print(matrixA.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = matrixA.filter(lambda tpl: tpl[2] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = matrixA.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909356\n"
     ]
    }
   ],
   "source": [
    "print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix is quite dense. 90% + of the values are not zeros, so we will use the BlockMatrix implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Using Sparks Distributed Matrixes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg.distributed import BlockMatrix, CoordinateMatrix, MatrixEntry\n",
    "from pyspark import sql\n",
    "\n",
    "sqlContext = sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixA1 = matrixA.map(lambda tpl: MatrixEntry(tpl[0], tpl[1], tpl[2]))\n",
    "matrixB1 = matrixB.map(lambda tpl: MatrixEntry(tpl[0], tpl[1], tpl[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MatrixEntry(0, 0, 3.0), MatrixEntry(0, 1, 9.0), MatrixEntry(0, 2, 10.0)]\n"
     ]
    }
   ],
   "source": [
    "print(matrixA1.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_matrixA = CoordinateMatrix(matrixA1)\n",
    "coord_matrixB = CoordinateMatrix(matrixB1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BlockMatrixA = coord_matrixA.toBlockMatrix()\n",
    "BlockMatrixB = coord_matrixB.toBlockMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = BlockMatrixA.multiply(BlockMatrixB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/matrix_mul.csv': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /matrix_mul.csv \n",
    "result.toIndexedRowMatrix().rows.sortBy(lambda x: x.index).saveAsTextFile(\"hdfs://master:9000/matrix_mul.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Pure MapReduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixA2 = matrixA.map(lambda tpl: (tpl[1],('A', tpl[0], tpl[2])))\n",
    "matrixB2 = matrixB.map(lambda tpl: (tpl[1],('B', tpl[0], tpl[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = matrixA2.join(matrixB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, (('A', 5050, 3), ('B', 0, 6)))\n"
     ]
    }
   ],
   "source": [
    "print(joined.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "frst_reduce = joined.map(lambda tpl:((tpl[1][0][1]*vals + tpl[1][1][1]), tpl[1][0][2]*tpl[1][1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50500000000, 0)\n"
     ]
    }
   ],
   "source": [
    "print(frst_reduce.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scnd_reduce = frst_reduce.reduceByKey(lambda accum, n: accum + n).map(lambda tpl: ((tpl[0]//vals), (tpl[0]%vals), tpl[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/matrix_mul2.csv': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /matrix_mul2.csv \n",
    "scnd_reduce.saveAsTextFile(\"hdfs://master:9000/matrix_mul2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
