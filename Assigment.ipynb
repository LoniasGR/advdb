{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Taxis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = sc.textFile('hdfs://master:9000/yellow_tripdata_1m.csv')\n",
    "vendors = sc.textFile('hdfs://master:9000/yellow_tripvendors_1m.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Average trip duration per hour it started ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def strip_time(time):\n",
    "    return datetime.strptime(time, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trp_dur = trips.map(lambda line: line.split(\",\")[1:3])\\\n",
    "        .map(lambda lst: (strip_time(lst[0]), strip_time(lst[1])))\\\n",
    "        .map(lambda lst: (lst[0].hour, lst[1]-lst[0]))\\\n",
    "        .mapValues(lambda x: (x, 1))\\\n",
    "        .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n",
    "        .map(lambda lst: (lst[0], lst[1][0].total_seconds()/(60*lst[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(23, 13.95847112523272), (22, 14.231797625637356), (21, 13.510855327392882), (20, 13.57589963636424), (19, 14.221208805985727), (18, 15.29045374860119), (17, 16.510825654408613), (16, 17.213072069374675), (15, 30.223498632126276), (14, 16.523138380789476), (13, 15.553918733195129), (12, 15.130881322885683), (11, 14.935821221905567), (10, 14.657939169698276), (9, 14.67010641976562), (8, 14.627504543367822), (7, 13.395006418527384), (6, 12.487420237563239), (5, 13.275583221175415), (4, 13.799857931121963), (3, 13.322282520526887), (2, 13.0356355926767), (1, 13.975069898907133), (0, 14.01779373736224)]\n"
     ]
    }
   ],
   "source": [
    "print(avg_trp_dur.top(24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trp_dur = avg_trp_dur.sortByKey(ascending=True)\\\n",
    "            .map(lambda tpl: {\"HourOfDay\":tpl[0], \"AverageTripTime\":tpl[1]})\n",
    "fieldnames=[\"HourOfDay\", \"AverageTripTime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "def writeRecords(records):\n",
    "    \"\"\"Write out CSV lines\"\"\"\n",
    "    output = StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=fieldnames, delimiter='\\t')\n",
    "    for record in records:\n",
    "        writer.writerow(record)\n",
    "    return [output.getvalue()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/03/20 00:23:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /average_trip.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /average_trip.csv \n",
    "avg_trp_dur.mapPartitions(writeRecords).saveAsTextFile(\"hdfs://master:9000/average_trip.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Max amount paid per vendor ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendors_key_val = vendors.map(lambda line: line.split(\",\")[0:2])\\\n",
    "                        .map(lambda lst: (lst[0],lst[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_key_val = trips.map(lambda line: line.split(\",\")[:])\\\n",
    "                    .map(lambda lst: (lst[0],lst[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinRDD = vendors_key_val.join(trips_key_val)\\\n",
    "                    .map(lambda tpl: tpl[1])\\\n",
    "                    .reduceByKey(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2', '99.99'), ('1', '995.3')]\n"
     ]
    }
   ],
   "source": [
    "print(joinRDD.top(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_by_vndr = joinRDD.sortByKey(ascending=True)\\\n",
    "            .map(lambda tpl: {\"VendorID\":tpl[0], \"MaxAmountPaid\":tpl[1]})\n",
    "fieldnames=[\"VendorID\", \"MaxAmountPaid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/03/20 00:29:00 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /max_amount.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /max_amount.csv \n",
    "max_by_vndr.mapPartitions(writeRecords).saveAsTextFile(\"hdfs://master:9000/max_amount.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Machine Learning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = trips.map(lambda line: line.split(\",\")[3:5])\\\n",
    "                .map(lambda lst: array([float(lst[0]), float(lst[1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model = KMeansModel(ml_data.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = KMeans.train(ml_data, 5, maxIterations=3, initializationMode=\"kmeans||\", initialModel=init_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-78.50386634,  40.61111272]),\n",
       " array([-0.0001304 ,  0.00050874]),\n",
       " array([-73.95393671,  40.69903131]),\n",
       " array([-73.99216792,  40.74268109]),\n",
       " array([-73.96113226,  40.77182523])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "centroids = []\n",
    "for c in clusters.clusterCenters:\n",
    "    centroids.append((i, c))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_centroids = sc.parallelize(centroids)\n",
    "p_centroids = p_centroids.map(lambda tpl: {\"ID\":tpl[0], \"Centroid\":tpl[1]})\n",
    "fieldnames=[\"ID\", \"Centroid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/centroids.csv': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /centroids.csv \n",
    "p_centroids.mapPartitions(writeRecords).saveAsTextFile(\"hdfs://master:9000/centroids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Page Rank ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = sc.textFile('hdfs://master:9000/web-Google.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pages.filter(lambda line: line[0]!='#')\\\n",
    "            .map(lambda line: line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 875713\n",
    "p0 = 0.5\n",
    "d = 0.85\n",
    "\n",
    "add1 = 1-d/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[916427, 916427, 916427, 916427, 916427]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
